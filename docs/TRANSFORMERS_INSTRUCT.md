æˆ‘æ¥è¯¦ç»†è§£é‡Šè¿™ä¸ªæ¨¡å‹æ¨¡å—çš„æ¶æ„å’ŒTransformeråŸç†ã€‚é¦–å…ˆè®©æˆ‘è¯»å–æ‰€æœ‰ç›¸å…³æ–‡ä»¶ã€‚

[5 tools called]

## æ¨¡å‹æ¨¡å—è¯¦ç»†è§£é‡Š

æ ¹æ®æ‚¨æä¾›çš„ä»£ç ï¼Œæˆ‘æ¥è¯¦ç»†è§£é‡Šè¿™ä¸ªTransformeræ¨¡å‹æ¨¡å—çš„æ¶æ„å’ŒåŸç†ã€‚

### ğŸ“š ä¸€ã€æ•´ä½“æ¶æ„æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„**Sequence-to-Sequence Transformer**å®ç°ï¼Œé‡‡ç”¨ç»å…¸çš„**ç¼–ç å™¨-è§£ç å™¨ï¼ˆEncoder-Decoderï¼‰**æ¶æ„ã€‚æ¨¡å—ç»“æ„æ¸…æ™°ï¼Œåˆ†ä¸ºå››ä¸ªå±‚æ¬¡ï¼š

```
model/
â”œâ”€â”€ embeddings.py      # Layer 1.1: è¯åµŒå…¥å’Œä½ç½®ç¼–ç 
â”œâ”€â”€ attention.py       # Layer 1.2: å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
â”œâ”€â”€ layers.py          # Layer 1.3: å‰é¦ˆç½‘ç»œå’ŒTransformerå—
â””â”€â”€ transformer.py     # Layer 1.4: å®Œæ•´çš„Transformeræ¨¡å‹
```

---

### ğŸ§© äºŒã€æ ¸å¿ƒç»„ä»¶è¯¦è§£

#### **1. åµŒå…¥å±‚ (embeddings.py)**

##### **PositionalEncoding - ä½ç½®ç¼–ç **
Transformeræ²¡æœ‰å¾ªç¯ç»“æ„ï¼Œæ— æ³•æ„ŸçŸ¥åºåˆ—é¡ºåºï¼Œå› æ­¤éœ€è¦ä½ç½®ç¼–ç æ¥æ³¨å…¥ä½ç½®ä¿¡æ¯ã€‚

**æ•°å­¦åŸç†ï¼š**
```
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

- `pos`: ä½ç½®ç´¢å¼•ï¼ˆ0, 1, 2, ...ï¼‰
- `i`: ç»´åº¦ç´¢å¼•
- ä½¿ç”¨æ­£å¼¦å’Œä½™å¼¦å‡½æ•°çš„ä¸åŒé¢‘ç‡æ¥ç¼–ç ä½ç½®

**å…³é”®ç‰¹æ€§ï¼š**
- ä½¿ç”¨ `register_buffer()` ç¡®ä¿åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­æ­£ç¡®å¤„ç†
- å›ºå®šä½ç½®ç¼–ç ï¼ˆä¸å‚ä¸è®­ç»ƒï¼‰ï¼Œä½†ä»£ç ä¹Ÿæ”¯æŒå¯å­¦ä¹ çš„ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆALiBiã€RoPEï¼‰

##### **TransformerEmbedding - è¯åµŒå…¥å±‚**
```python
åµŒå…¥è¾“å‡º = token_embedding(x) * sqrt(d_model) + positional_encoding
```
- è¯åµŒå…¥ä¹˜ä»¥ `sqrt(d_model)` æ˜¯ä¸ºäº†å¹³è¡¡åµŒå…¥å€¼å’Œä½ç½®ç¼–ç çš„å°ºåº¦
- æ”¯æŒå¤šç§ä½ç½®ç¼–ç æ–¹æ¡ˆï¼šsinusoidalï¼ˆæ­£ä½™å¼¦ï¼‰ã€ALiBiã€RoPE

---

#### **2. æ³¨æ„åŠ›æœºåˆ¶ (attention.py)**

##### **ScaledDotProductAttention - ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›**

**æ ¸å¿ƒå…¬å¼ï¼š**
```
Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V
```

**å·¥ä½œåŸç†ï¼š**
1. **è®¡ç®—ç›¸ä¼¼åº¦**ï¼š`QK^T` å¾—åˆ°æ¯ä¸ªqueryä¸æ‰€æœ‰keyçš„ç‚¹ç§¯ï¼ˆæ³¨æ„åŠ›åˆ†æ•°ï¼‰
2. **ç¼©æ”¾**ï¼šé™¤ä»¥ `sqrt(d_k)` é˜²æ­¢ç‚¹ç§¯å€¼è¿‡å¤§å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±
3. **Softmaxå½’ä¸€åŒ–**ï¼šå¾—åˆ°æ³¨æ„åŠ›æƒé‡ï¼ˆæ¦‚ç‡åˆ†å¸ƒï¼‰
4. **åŠ æƒæ±‚å’Œ**ï¼šç”¨æƒé‡å¯¹valueåŠ æƒæ±‚å’Œ

**ä¼˜åŒ–æŠ€æœ¯ï¼š**
- ä¼˜å…ˆä½¿ç”¨ **PyTorch SDPA**ï¼ˆScaled Dot Product Attentionï¼‰æˆ– **xFormers** å®ç°é«˜æ•ˆè®¡ç®—
- æ”¯æŒ **Padding Mask**ï¼ˆå¿½ç•¥å¡«å……ä½ç½®ï¼‰å’Œ **Causal Mask**ï¼ˆé˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯ï¼‰
- æ”¯æŒ **ALiBiåç½®**ï¼ˆç›¸å¯¹ä½ç½®ç¼–ç çš„ä¸€ç§å®ç°ï¼‰

##### **MultiHeadAttention - å¤šå¤´æ³¨æ„åŠ›**

**æ ¸å¿ƒæ€æƒ³ï¼š** å°†è¾“å…¥æŠ•å½±åˆ°å¤šä¸ªå­ç©ºé—´ï¼Œåœ¨æ¯ä¸ªå­ç©ºé—´å¹¶è¡Œè®¡ç®—æ³¨æ„åŠ›ï¼Œæœ€ååˆå¹¶ã€‚

**æ•°å­¦è¡¨ç¤ºï¼š**
```
MultiHead(Q,K,V) = Concat(head_1, ..., head_h) W^O
å…¶ä¸­ head_i = Attention(Q W_i^Q, K W_i^K, V W_i^V)
```

**ä»£ç å®ç°æµç¨‹ï¼š**
```python
# 1. çº¿æ€§æŠ•å½±
Q = W_q(query)  # (batch, seq_len, d_model)
K = W_k(key)
V = W_v(value)

# 2. åˆ†å¤´ï¼ˆreshapeï¼‰
Q = split_heads(Q)  # (batch, num_heads, seq_len, d_k)
# d_k = d_model / num_heads

# 3. è®¡ç®—æ³¨æ„åŠ›
attn_output, weights = attention(Q, K, V, mask)

# 4. åˆå¹¶å¤šå¤´
output = combine_heads(attn_output)  # (batch, seq_len, d_model)
output = W_o(output)
```

**é«˜çº§ç‰¹æ€§ï¼š**
- **å…¨å±€æ³¨æ„åŠ›**ï¼ˆ`attention_type='full'`ï¼‰ï¼šæ¯ä¸ªä½ç½®å¯ä»¥å…³æ³¨æ‰€æœ‰ä½ç½®
- **å±€éƒ¨æ»‘çª—æ³¨æ„åŠ›**ï¼ˆ`attention_type='local'`ï¼‰ï¼šåªå…³æ³¨çª—å£å†…çš„ä½ç½®ï¼Œé™ä½è®¡ç®—å¤æ‚åº¦
- **RoPEæ—‹è½¬ä½ç½®ç¼–ç **ï¼šç›´æ¥ä½œç”¨äºQ/Kå‘é‡ï¼Œä¿æŒç›¸å¯¹ä½ç½®ä¸å˜æ€§
- **ALiBiåç½®**ï¼šé€šè¿‡é¢„è®¡ç®—çš„æ–œç‡çŸ©é˜µè°ƒæ•´æ³¨æ„åŠ›åˆ†æ•°

---

#### **3. å‰é¦ˆç½‘ç»œå±‚ (layers.py)**

##### **PositionwiseFeedForward - ä½ç½®å‰é¦ˆç½‘ç»œ**

**ç»“æ„ï¼š**
```
FFN(x) = Linear2(Dropout(Activation(Linear1(x))))
      = W2 * Activation(W1 * x + b1) + b2
```

- **ç»´åº¦å˜æ¢**ï¼š`d_model â†’ d_ff â†’ d_model`ï¼ˆé€šå¸¸ `d_ff = 4 * d_model`ï¼‰
- **æ¿€æ´»å‡½æ•°**ï¼šæ”¯æŒReLUæˆ–GELU
- **ä½œç”¨**ï¼šå¯¹æ¯ä¸ªä½ç½®ç‹¬ç«‹åœ°è¿›è¡Œéçº¿æ€§å˜æ¢ï¼Œå¢å¼ºæ¨¡å‹è¡¨è¾¾èƒ½åŠ›

##### **TransformerBlock - Transformerå—**

è¿™æ˜¯Transformerçš„åŸºæœ¬å•å…ƒï¼ŒåŒ…å«ï¼š

**ç¼–ç å™¨å—ï¼š**
```
1. å¤šå¤´è‡ªæ³¨æ„åŠ› (Self-Attention)
   output = LayerNorm(x + Dropout(MultiHeadAttention(x, x, x, mask)))

2. å‰é¦ˆç½‘ç»œ (FFN)
   output = LayerNorm(output + Dropout(FFN(output)))
```

**è§£ç å™¨å—ï¼š**
```
1. å¸¦é®ç½©çš„å¤šå¤´è‡ªæ³¨æ„åŠ› (Masked Self-Attention)
   output = LayerNorm(x + Dropout(MultiHeadAttention(x, x, x, tgt_mask)))

2. äº¤å‰æ³¨æ„åŠ› (Cross-Attention)
   output = LayerNorm(output + Dropout(MultiHeadAttention(
       output,           # Queryæ¥è‡ªdecoder
       encoder_output,   # Keyæ¥è‡ªencoder
       encoder_output,   # Valueæ¥è‡ªencoder
       src_mask
   )))

3. å‰é¦ˆç½‘ç»œ (FFN)
   output = LayerNorm(output + Dropout(FFN(output)))
```

**å…³é”®è®¾è®¡ï¼š**
- **æ®‹å·®è¿æ¥**ï¼ˆResidual Connectionï¼‰ï¼š`x + Sublayer(x)` è§£å†³æ·±å±‚ç½‘ç»œæ¢¯åº¦æ¶ˆå¤±
- **å±‚å½’ä¸€åŒ–**ï¼ˆLayer Normalizationï¼‰ï¼šç¨³å®šè®­ç»ƒï¼ŒåŠ é€Ÿæ”¶æ•›

---

#### **4. å®Œæ•´æ¨¡å‹ (transformer.py)**

##### **TransformerEncoder - ç¼–ç å™¨**
- å †å  `num_layers` ä¸ªç¼–ç å™¨å—
- è¾“å…¥æºåºåˆ—ï¼Œè¾“å‡ºä¸Šä¸‹æ–‡è¡¨ç¤º
- æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆGradient Checkpointingï¼‰èŠ‚çœæ˜¾å­˜

##### **TransformerDecoder - è§£ç å™¨**
- å †å  `num_layers` ä¸ªè§£ç å™¨å—
- æ¥æ”¶ç¼–ç å™¨è¾“å‡ºå’Œç›®æ ‡åºåˆ—
- ä½¿ç”¨å› æœæ©ç ï¼ˆCausal Maskï¼‰å®ç°è‡ªå›å½’ç”Ÿæˆ

##### **Transformer - å®Œæ•´æ¨¡å‹**

**å‰å‘ä¼ æ’­æµç¨‹ï¼š**
```python
def forward(src, tgt):
    # 1. ç”Ÿæˆæ©ç 
    src_mask = make_src_mask(src)  # Padding Mask
    tgt_mask = make_tgt_mask(tgt)  # Causal Mask + Padding Mask
    
    # 2. åµŒå…¥
    src_emb = src_embedding(src)
    tgt_emb = tgt_embedding(tgt)
    
    # 3. ç¼–ç 
    encoder_output = encoder(src_emb, src_mask)
    
    # 4. è§£ç 
    decoder_output = decoder(tgt_emb, encoder_output, src_mask, tgt_mask)
    
    # 5. è¾“å‡ºæŠ•å½±
    output = output_projection(decoder_output)  # (batch, seq_len, vocab_size)
    
    return output
```

**é‡è¦æ©ç æœºåˆ¶ï¼š**

1. **Padding Mask**ï¼ˆæºåºåˆ—ï¼‰ï¼š
```python
src_mask = (src != pad_idx)  # Trueè¡¨ç¤ºæœ‰æ•ˆä½ç½®ï¼ŒFalseè¡¨ç¤ºå¡«å……
```

2. **Causal Mask**ï¼ˆç›®æ ‡åºåˆ—ï¼‰ï¼š
```python
# ä¸‹ä¸‰è§’çŸ©é˜µï¼Œé˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯
causal_mask = [[1, 0, 0],
               [1, 1, 0],
               [1, 1, 1]]
```

---

### âš™ï¸ ä¸‰ã€Transformerå·¥ä½œåŸç†

#### **è®­ç»ƒé˜¶æ®µï¼ˆTeacher Forcingï¼‰ï¼š**
```
è¾“å…¥: src = "I love AI"  tgt = "<BOS> æˆ‘ çˆ± AI <EOS>"
è¾“å‡º: logits å¯¹åº”é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒ

æŸå¤±è®¡ç®—:
- è¾“å…¥åˆ°decoder: "<BOS> æˆ‘ çˆ± AI"
- é¢„æµ‹ç›®æ ‡:      "æˆ‘ çˆ± AI <EOS>"
- ä½¿ç”¨äº¤å‰ç†µæŸå¤±æ¯”è¾ƒé¢„æµ‹å’ŒçœŸå®æ ‡ç­¾
```

#### **æ¨ç†é˜¶æ®µï¼ˆè‡ªå›å½’ç”Ÿæˆï¼‰ï¼š**
```python
# 1. ç¼–ç æºåºåˆ—
encoder_output = model.encode(src)

# 2. åˆå§‹åŒ–ç›®æ ‡åºåˆ—
tgt = [<BOS>]

# 3. é€æ­¥ç”Ÿæˆ
for step in range(max_len):
    # è§£ç å½“å‰åºåˆ—
    logits = model.decode(tgt, encoder_output, src_mask)
    
    # å–æœ€åä¸€ä¸ªä½ç½®çš„é¢„æµ‹
    next_token = logits[:, -1, :].argmax(dim=-1)
    
    # æ·»åŠ åˆ°åºåˆ—
    tgt.append(next_token)
    
    # å¦‚æœç”Ÿæˆ<EOS>åˆ™åœæ­¢
    if next_token == <EOS>:
        break
```

---

### ğŸš€ å››ã€ä»£ç ä¸­çš„é«˜çº§ä¼˜åŒ–

1. **æ¢¯åº¦æ£€æŸ¥ç‚¹** (`use_checkpoint=True`)
   - è®­ç»ƒæ—¶ä¸ä¿å­˜ä¸­é—´æ¿€æ´»å€¼ï¼Œåå‘ä¼ æ’­æ—¶é‡æ–°è®¡ç®—
   - ç”¨æ—¶é—´æ¢ç©ºé—´ï¼Œå¤§å¹…é™ä½æ˜¾å­˜å ç”¨

2. **é«˜æ•ˆæ³¨æ„åŠ›å®ç°**
   - PyTorch SDPAï¼šèåˆkernelï¼Œå‡å°‘å†…å­˜è¯»å†™
   - xFormersï¼šå†…å­˜é«˜æ•ˆæ³¨æ„åŠ›ï¼Œæ”¯æŒè¶…é•¿åºåˆ—
   - å±€éƒ¨æ»‘çª—æ³¨æ„åŠ›ï¼šé™ä½ O(nÂ²) å¤æ‚åº¦åˆ° O(nÃ—window)

3. **å¤šç§ä½ç½®ç¼–ç **
   - **Sinusoidal**ï¼šå›ºå®šçš„æ­£ä½™å¼¦å‡½æ•°
   - **ALiBi**ï¼šæ³¨æ„åŠ›åç½®ï¼Œæ— éœ€ä½ç½®åµŒå…¥
   - **RoPE**ï¼šæ—‹è½¬ä½ç½®ç¼–ç ï¼Œä¿æŒç›¸å¯¹ä½ç½®ä¿¡æ¯

4. **äº¤å‰æ³¨æ„åŠ›ä¸‹é‡‡æ ·** (`cross_downsample`)
   - å¯¹ç¼–ç å™¨è¾“å‡ºè¿›è¡Œæ± åŒ–ï¼Œå‡å°‘è§£ç å™¨è®¡ç®—é‡
   - é€‚ç”¨äºæºåºåˆ—è¿œé•¿äºç›®æ ‡åºåˆ—çš„åœºæ™¯

5. **Xavieråˆå§‹åŒ–**
   - ç»Ÿä¸€åˆå§‹åŒ–æ‰€æœ‰å‚æ•°ï¼ŒåŠ é€Ÿæ”¶æ•›

---

### ğŸ“Š äº”ã€æ¨¡å‹å‚æ•°é‡ä¼°ç®—

ä»¥æ ‡å‡†é…ç½®ä¸ºä¾‹ï¼š
```
d_model=512, num_heads=8, d_ff=2048, num_layers=6
vocab_size=50000

åµŒå…¥å±‚:    50000 Ã— 512 Ã— 2 â‰ˆ 51M
ç¼–ç å™¨:    6 Ã— (4Ã—512Â²) â‰ˆ 6.3M
è§£ç å™¨:    6 Ã— (4Ã—512Â² + äº¤å‰æ³¨æ„åŠ›) â‰ˆ 9.4M
è¾“å‡ºå±‚:    512 Ã— 50000 â‰ˆ 25.6M

æ€»å‚æ•°é‡: çº¦ 92M
```

---

### ğŸ¯ å…­ã€æ€»ç»“

è¿™ä¸ªå®ç°æ˜¯ä¸€ä¸ª**å·¥ä¸šçº§çš„Transformeræ¨¡å‹**ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

âœ… **æ¶æ„å®Œæ•´**ï¼šåŒ…å«ç¼–ç å™¨ã€è§£ç å™¨ã€å¤šç§æ³¨æ„åŠ›æœºåˆ¶  
âœ… **é«˜æ•ˆä¼˜åŒ–**ï¼šæ”¯æŒSDPAã€xFormersã€æ¢¯åº¦æ£€æŸ¥ç‚¹ã€æ»‘çª—æ³¨æ„åŠ›  
âœ… **çµæ´»é…ç½®**ï¼šæ”¯æŒå¤šç§ä½ç½®ç¼–ç ã€æ¿€æ´»å‡½æ•°ã€æ³¨æ„åŠ›æ¨¡å¼  
âœ… **åˆ†å¸ƒå¼å‹å¥½**ï¼šæ­£ç¡®ä½¿ç”¨register_bufferï¼Œå…¼å®¹DeepSpeed/DDP  
âœ… **ä»£ç æ¸…æ™°**ï¼šå±‚æ¬¡åˆ†æ˜ï¼Œæ³¨é‡Šè¯¦ç»†ï¼Œæ˜“äºæ‰©å±•å’Œç»´æŠ¤  

è¿™æ˜¯ä¸€ä¸ªéå¸¸é€‚åˆå­¦ä¹ å’Œå®æˆ˜çš„Transformerå®ç°ï¼