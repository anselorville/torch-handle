# Torch-Handle: å¤§è§„æ¨¡ Transformer åˆ†å¸ƒå¼è®­ç»ƒå·¥ç¨‹æ¡†æ¶

> åŸºäº PyTorch 1.10+ã€torchrun ä¸ DeepSpeed çš„å·¥ä¸šçº§å¤§æ¨¡å‹è®­ç»ƒå®è·µé¡¹ç›®

## ğŸ“– é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ª**ä»é›¶å¼€å§‹æ‰‹æ“ Transformer**ï¼Œå¹¶ç»“åˆ**åˆ†å¸ƒå¼è®­ç»ƒï¼ˆDDPï¼‰**å’Œ**å¤§è§„æ¨¡ä¼˜åŒ–ï¼ˆDeepSpeed ZeROï¼‰**çš„å®Œæ•´å·¥ç¨‹å®è·µæ¡†æ¶ã€‚é¡¹ç›®æ—¨åœ¨å¸®åŠ©å¼€å‘è€…æ·±å…¥ç†è§£å¤§æ¨¡å‹è®­ç»ƒçš„åº•å±‚åŸç†ï¼ŒæŒæ¡ä»å•å¡åˆ°å¤šå¡ã€ä» DDP åˆ° DeepSpeed çš„å®Œæ•´æŠ€æœ¯æ ˆã€‚

### æ ¸å¿ƒç‰¹æ€§

- âœ… **å®Œæ•´ Transformer å®ç°**ï¼šä»ä½ç½®ç¼–ç ã€å¤šå¤´æ³¨æ„åŠ›åˆ°å®Œæ•´çš„ Encoder-Decoder æ¶æ„
- âœ… **åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ**ï¼šåŸºäº torchrun çš„ DDP å®ç°ï¼Œæ”¯æŒå¤šå¡/å¤šèŠ‚ç‚¹è®­ç»ƒ
- âœ… **DeepSpeed é›†æˆ**ï¼šæ”¯æŒ ZeRO Stage 1/2/3ï¼Œå®ç°è¶…å¤§æ¨¡å‹è®­ç»ƒ
- âœ… **å†…å­˜ä¼˜åŒ–æŠ€æœ¯**ï¼šæ¢¯åº¦æ£€æŸ¥ç‚¹ã€æ··åˆç²¾åº¦è®­ç»ƒã€å‚æ•°å¸è½½
- âœ… **æ€§èƒ½åˆ†æå·¥å…·**ï¼šé›†æˆ PyTorch Profilerï¼Œè¯Šæ–­è®­ç»ƒç“¶é¢ˆ
- âœ… **å·¥ç¨‹æœ€ä½³å®è·µ**ï¼šæ¨¡å—åŒ–è®¾è®¡ã€å®¹é”™æœºåˆ¶ã€Checkpoint ç®¡ç†

---

## ğŸ—ï¸ é¡¹ç›®æ¶æ„ï¼šåº•å±‚åˆ°ä¸Šå±‚çš„æ¨¡å—è®¾è®¡

æœ¬é¡¹ç›®é‡‡ç”¨**åˆ†å±‚æ¨¡å—åŒ–è®¾è®¡**ï¼Œä¸¥æ ¼éµå¾ªä»åº•å±‚ç»„ä»¶åˆ°ä¸Šå±‚è®­ç»ƒé€»è¾‘çš„æ„å»ºé¡ºåºï¼š

```
torch-handle/
â”‚
â”œâ”€â”€ ğŸ“ model/                          # ã€Layer 1: åº•å±‚æ¨¡å‹ç»„ä»¶ã€‘
â”‚   â”œâ”€â”€ embeddings.py                  # è¯åµŒå…¥ + ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰
â”‚   â”œâ”€â”€ attention.py                   # å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Attentionï¼‰
â”‚   â”œâ”€â”€ layers.py                      # FFNã€æ®‹å·®è¿æ¥ã€LayerNorm
â”‚   â””â”€â”€ transformer.py                 # å®Œæ•´ Transformerï¼ˆEncoder + Decoderï¼‰
â”‚
â”œâ”€â”€ ğŸ“ data/                           # ã€Layer 2: æ•°æ®å¤„ç†å±‚ã€‘
â”‚   â”œâ”€â”€ dataset.py                     # è‡ªå®šä¹‰æ•°æ®é›†ï¼ˆæ”¯æŒ Tokenizationï¼‰
â”‚   â””â”€â”€ dataloader.py                  # åˆ†å¸ƒå¼æ•°æ®åŠ è½½å™¨ï¼ˆDistributedSamplerï¼‰
â”‚
â”œâ”€â”€ ğŸ“ train/                          # ã€Layer 3: è®­ç»ƒé€»è¾‘å±‚ã€‘
â”‚   â”œâ”€â”€ ddp_trainer.py                 # DDP è®­ç»ƒå™¨ï¼ˆå•æœºå¤šå¡ / å¤šèŠ‚ç‚¹ï¼‰
â”‚   â””â”€â”€ deepspeed_trainer.py           # DeepSpeed è®­ç»ƒå™¨ï¼ˆZeRO ä¼˜åŒ–ï¼‰
â”‚
â”œâ”€â”€ ğŸ“ utils/                          # ã€Layer 4: å·¥å…·ä¸ä¼˜åŒ–ã€‘
â”‚   â”œâ”€â”€ checkpoint.py                  # Checkpoint ä¿å­˜/åŠ è½½/å®¹é”™
â”‚   â”œâ”€â”€ profiler.py                    # æ€§èƒ½åˆ†æå·¥å…·ï¼ˆPyTorch Profilerï¼‰
â”‚   â””â”€â”€ gradient_checkpointing.py      # æ¢¯åº¦æ£€æŸ¥ç‚¹è¾…åŠ©å‡½æ•°
â”‚
â”œâ”€â”€ ğŸ“ config/                         # ã€Layer 5: é…ç½®ç®¡ç†ã€‘
â”‚   â”œâ”€â”€ ds_config_stage1.json          # DeepSpeed Stage 1 é…ç½®
â”‚   â”œâ”€â”€ ds_config_stage2.json          # DeepSpeed Stage 2 é…ç½®
â”‚   â”œâ”€â”€ ds_config_stage3.json          # DeepSpeed Stage 3 é…ç½®ï¼ˆLLM è®­ç»ƒï¼‰
â”‚   â””â”€â”€ model_config.yaml              # æ¨¡å‹è¶…å‚æ•°é…ç½®
â”‚
â”œâ”€â”€ ğŸ“ scripts/                        # ã€Layer 6: å¯åŠ¨è„šæœ¬ã€‘
â”‚   â”œâ”€â”€ train_ddp_single_node.sh       # å•æœºå¤šå¡ DDP è®­ç»ƒ
â”‚   â”œâ”€â”€ train_ddp_multi_node.sh        # å¤šèŠ‚ç‚¹ DDP è®­ç»ƒ
â”‚   â””â”€â”€ train_deepspeed.sh             # DeepSpeed è®­ç»ƒï¼ˆæ”¯æŒ ZeROï¼‰
â”‚
â”œâ”€â”€ main.py                            # ã€ä¸»å…¥å£ã€‘è®­ç»ƒæµç¨‹ç¼–æ’
â”œâ”€â”€ requirements.txt                   # ä¾èµ–ç®¡ç†
â””â”€â”€ README.md                          # é¡¹ç›®æ–‡æ¡£
```

---

## ğŸ“š æ¨¡å—è¯¦è§£ï¼šä»åº•å±‚åˆ°ä¸Šå±‚çš„æ„å»ºé€»è¾‘

### Layer 1: åº•å±‚æ¨¡å‹ç»„ä»¶ (`model/`)

è¿™æ˜¯é¡¹ç›®çš„**æ ¸å¿ƒ**ï¼Œå®ç° Transformer çš„æ‰€æœ‰åŸºç¡€ç»„ä»¶ã€‚

#### 1.1 `embeddings.py` - è¯åµŒå…¥ä¸ä½ç½®ç¼–ç 
**æ„å»ºé¡ºåºï¼šç¬¬ 1 æ­¥**

- **åŠŸèƒ½**ï¼š
  - å®ç° `nn.Embedding` çš„è¯åµŒå…¥å±‚
  - å®ç°åŸºäºæ­£å¼¦/ä½™å¼¦å‡½æ•°çš„ **Positional Encoding**
  - ä½¿ç”¨ `register_buffer()` ç¡®ä¿ä½ç½®ç¼–ç åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹çš„æ­£ç¡®æ€§

- **å…³é”®æŠ€æœ¯**ï¼š
  - ä½ç½®ç¼–ç å…¬å¼ï¼š$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})$
  - éè®­ç»ƒå‚æ•°çš„ Buffer ç®¡ç†ï¼ˆé€‚é… DDP/DeepSpeedï¼‰

#### 1.2 `attention.py` - å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
**æ„å»ºé¡ºåºï¼šç¬¬ 2 æ­¥**

- **åŠŸèƒ½**ï¼š
  - å®ç° **Scaled Dot-Product Attention**
  - å®ç° **Multi-Head Attention** çš„åˆ†å¤´å¹¶è¡Œè®¡ç®—
  - æ”¯æŒ Padding Mask å’Œ Causal Mask

- **å…³é”®æŠ€æœ¯**ï¼š
  - æ³¨æ„åŠ›å…¬å¼ï¼š$\text{Attention}(Q, K, V) = \text{Softmax}(\frac{QK^T}{\sqrt{d_k}})V$
  - å¼ é‡é‡å¡‘ï¼š`(batch, seq_len, d_model)` â†’ `(batch, num_heads, seq_len, d_k)`

#### 1.3 `layers.py` - å‰é¦ˆç½‘ç»œä¸æ®‹å·®è¿æ¥
**æ„å»ºé¡ºåºï¼šç¬¬ 3 æ­¥**

- **åŠŸèƒ½**ï¼š
  - å®ç° **Position-wise Feed-Forward Networks (FFN)**
  - å®ç° **æ®‹å·®è¿æ¥ + Layer Normalization**
  - æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆGradient Checkpointingï¼‰

- **å…³é”®æŠ€æœ¯**ï¼š
  - FFN ç»“æ„ï¼š`Linear(d_model â†’ d_ff) â†’ ReLU/GELU â†’ Linear(d_ff â†’ d_model)`
  - æ®‹å·®è¿æ¥ï¼š`LayerNorm(x + Sublayer(x))`

#### 1.4 `transformer.py` - å®Œæ•´ Transformer æ¶æ„
**æ„å»ºé¡ºåºï¼šç¬¬ 4 æ­¥**

- **åŠŸèƒ½**ï¼š
  - ç»„è£… **Encoder Block** å’Œ **Decoder Block**
  - å †å å¤šå±‚ Transformer å±‚
  - å®ç° Seq2Seq ä»»åŠ¡çš„è¾“å‡ºå±‚

- **ä¾èµ–å…³ç³»**ï¼š
  - ä¾èµ– `embeddings.py`ã€`attention.py`ã€`layers.py`

---

### Layer 2: æ•°æ®å¤„ç†å±‚ (`data/`)

#### 2.1 `dataset.py` - è‡ªå®šä¹‰æ•°æ®é›†
**æ„å»ºé¡ºåºï¼šç¬¬ 5 æ­¥**

- **åŠŸèƒ½**ï¼š
  - å®ç° `torch.utils.data.Dataset` æ¥å£
  - æ”¯æŒæ–‡æœ¬ Tokenizationï¼ˆä½¿ç”¨ `torchtext` æˆ– `tokenizers`ï¼‰
  - æ”¯æŒ Seq2Seq æ•°æ®å¯¹ï¼ˆæºè¯­è¨€ + ç›®æ ‡è¯­è¨€ï¼‰

#### 2.2 `dataloader.py` - åˆ†å¸ƒå¼æ•°æ®åŠ è½½
**æ„å»ºé¡ºåºï¼šç¬¬ 6 æ­¥**

- **åŠŸèƒ½**ï¼š
  - å°è£… `torch.utils.data.DataLoader`
  - ä½¿ç”¨ `DistributedSampler` å®ç°æ•°æ®åˆ†ç‰‡
  - æ”¯æŒ `pin_memory` å’Œå¼‚æ­¥ä¼ è¾“

- **å…³é”®æŠ€æœ¯**ï¼š
  - **å¿…é¡»åœ¨æ¯ä¸ª epoch è°ƒç”¨ `sampler.set_epoch(epoch)`**ï¼ˆé¿å…æ•°æ®åˆ†ç‰‡é‡å¤ï¼‰
  - ä¼˜åŒ–ï¼š`num_workers > 0`ã€`pin_memory=True`ã€`non_blocking=True`

---

### Layer 3: è®­ç»ƒé€»è¾‘å±‚ (`train/`)

#### 3.1 `ddp_trainer.py` - DDP è®­ç»ƒå™¨
**æ„å»ºé¡ºåºï¼šç¬¬ 7 æ­¥**

- **åŠŸèƒ½**ï¼š
  - å®ç° `ddp_setup()` å’Œ `destroy_process_group()`
  - å°è£…æ¨¡å‹ä¸º `DDP(model, device_ids=[local_rank])`
  - å®ç°è®­ç»ƒå¾ªç¯ï¼šå‰å‘ â†’ æŸå¤± â†’ åå‘ â†’ ä¼˜åŒ–
  - å®ç° Rank 0 ç‹¬å çš„ Checkpoint ä¿å­˜

- **å…³é”®æŠ€æœ¯**ï¼š
  - åˆå§‹åŒ–è¿›ç¨‹ç»„ï¼š`dist.init_process_group(backend="nccl")`
  - è·å–ç¯å¢ƒå˜é‡ï¼š`RANK`ã€`LOCAL_RANK`ã€`WORLD_SIZE`ï¼ˆç”± torchrun æ³¨å…¥ï¼‰

#### 3.2 `deepspeed_trainer.py` - DeepSpeed è®­ç»ƒå™¨
**æ„å»ºé¡ºåºï¼šç¬¬ 8 æ­¥**

- **åŠŸèƒ½**ï¼š
  - ä½¿ç”¨ `deepspeed.initialize()` å°è£…æ¨¡å‹å’Œä¼˜åŒ–å™¨
  - æ”¯æŒ ZeRO Stage 1/2/3
  - æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16/BF16ï¼‰
  - æ”¯æŒå‚æ•°/ä¼˜åŒ–å™¨çŠ¶æ€å¸è½½åˆ° CPU/NVMe

- **å…³é”®æŠ€æœ¯**ï¼š
  - è¯»å– `ds_config.json` é…ç½®
  - ä½¿ç”¨ `engine.backward()` å’Œ `engine.step()` æ›¿ä»£åŸç”Ÿ PyTorch

---

### Layer 4: å·¥å…·ä¸ä¼˜åŒ– (`utils/`)

#### 4.1 `checkpoint.py` - Checkpoint ç®¡ç†
**æ„å»ºé¡ºåºï¼šç¬¬ 9 æ­¥**

- **åŠŸèƒ½**ï¼š
  - ä¿å­˜/åŠ è½½æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€è®­ç»ƒçŠ¶æ€
  - æ”¯æŒ DDP å’Œ DeepSpeed çš„ Checkpoint æ ¼å¼
  - å®ç°è®­ç»ƒæ¢å¤ï¼ˆä»ä¸­æ–­ç‚¹ç»§ç»­ï¼‰

#### 4.2 `profiler.py` - æ€§èƒ½åˆ†æ
**æ„å»ºé¡ºåºï¼šç¬¬ 10 æ­¥**

- **åŠŸèƒ½**ï¼š
  - é›†æˆ PyTorch Profiler
  - å¯¼å‡º TensorBoard æˆ– Chrome Trace æ ¼å¼
  - åˆ†æ CPU/GPU æ—¶é—´ã€é€šä¿¡/è®¡ç®—é‡å 

#### 4.3 `gradient_checkpointing.py` - æ¢¯åº¦æ£€æŸ¥ç‚¹
**æ„å»ºé¡ºåºï¼šç¬¬ 11 æ­¥**

- **åŠŸèƒ½**ï¼š
  - å°è£… `torch.utils.checkpoint.checkpoint()`
  - åº”ç”¨äº Transformer Blockï¼ˆMHA + FFNï¼‰
  - æ¨èä½¿ç”¨ `use_reentrant=False` æ¨¡å¼

---

### Layer 5: é…ç½®ç®¡ç† (`config/`)

**æ„å»ºé¡ºåºï¼šç¬¬ 12 æ­¥**

- **`ds_config_stage1.json`**ï¼šä»…åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆé€‚åˆä¸­ç­‰æ¨¡å‹ï¼‰
- **`ds_config_stage2.json`**ï¼šåˆ†ç‰‡ä¼˜åŒ–å™¨ + æ¢¯åº¦ï¼ˆå†…å­˜èŠ‚çœ 3-4xï¼‰
- **`ds_config_stage3.json`**ï¼šå…¨åˆ†ç‰‡ï¼ˆP + G + Oï¼‰ï¼Œé€‚åˆ LLM è®­ç»ƒ

---

### Layer 6: å¯åŠ¨è„šæœ¬ (`scripts/`)

**æ„å»ºé¡ºåºï¼šç¬¬ 13 æ­¥**

æä¾›ä¸€é”®å¯åŠ¨çš„ Bash è„šæœ¬ï¼Œå°è£…å¤æ‚çš„ `torchrun` å‘½ä»¤ã€‚

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒå‡†å¤‡

```bash
# 1. å…‹éš†é¡¹ç›®
git clone <your-repo-url>
cd torch-handle

# 2. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 3. ï¼ˆå¯é€‰ï¼‰å®‰è£… DeepSpeed
pip install deepspeed
```

### å•æœºå¤šå¡è®­ç»ƒï¼ˆDDPï¼‰

```bash
# ä½¿ç”¨ 8 å¼  GPU
bash scripts/train_ddp_single_node.sh
```

ç­‰ä»·äºï¼š
```bash
torchrun --nproc_per_node=8 main.py --mode ddp --epochs 10 --batch_size 32
```

### å¤šèŠ‚ç‚¹è®­ç»ƒï¼ˆDDPï¼‰

åœ¨**æ¯ä¸ªèŠ‚ç‚¹**ä¸Šåˆ†åˆ«æ‰§è¡Œï¼š

```bash
# èŠ‚ç‚¹ 0ï¼ˆä¸»èŠ‚ç‚¹ï¼‰
bash scripts/train_ddp_multi_node.sh --node_rank 0

# èŠ‚ç‚¹ 1
bash scripts/train_ddp_multi_node.sh --node_rank 1
```

### DeepSpeed è®­ç»ƒï¼ˆZeRO Stage 3ï¼‰

```bash
bash scripts/train_deepspeed.sh
```

ç­‰ä»·äºï¼š
```bash
torchrun --nproc_per_node=8 main.py \
    --mode deepspeed \
    --deepspeed config/ds_config_stage3.json \
    --epochs 10
```

---

## ğŸ“Š æ€§èƒ½åˆ†æ

å¯ç”¨ PyTorch Profiler è¿›è¡Œæ€§èƒ½è¯Šæ–­ï¼š

```bash
python main.py --mode ddp --enable_profiler --profile_steps 10
```

æŸ¥çœ‹ TensorBoardï¼š
```bash
tensorboard --logdir=./logs
```

å…³æ³¨æŒ‡æ ‡ï¼š
- **é€šä¿¡/è®¡ç®—é‡å **ï¼šNCCL AllReduce æ˜¯å¦ä¸ CUDA Kernel å¹¶è¡Œ
- **æ•°æ®åŠ è½½æ•ˆç‡**ï¼šCPU wait_time æ˜¯å¦è¿‡é«˜
- **å†…å­˜å³°å€¼**ï¼šéªŒè¯æ¢¯åº¦æ£€æŸ¥ç‚¹çš„æ•ˆæœ

---

## ğŸ¯ å­¦ä¹ è·¯å¾„å»ºè®®

### åˆçº§ï¼šç†è§£æ ¸å¿ƒç»„ä»¶
1. é˜…è¯» `model/embeddings.py` å’Œ `model/attention.py`
2. ç†è§£ Transformer çš„åŸºæœ¬è®¡ç®—æµç¨‹
3. åœ¨å• GPU ä¸Šè¿è¡Œ `python main.py --mode single`

### ä¸­çº§ï¼šæŒæ¡åˆ†å¸ƒå¼è®­ç»ƒ
1. å­¦ä¹  `train/ddp_trainer.py` çš„ DDP æ ·æ¿ä»£ç 
2. ç†è§£ `DistributedSampler` çš„æ•°æ®åˆ†ç‰‡æœºåˆ¶
3. è¿è¡Œå•æœºå¤šå¡è®­ç»ƒå¹¶è§‚å¯ŸåŠ é€Ÿæ¯”

### é«˜çº§ï¼šå¤§è§„æ¨¡æ¨¡å‹ä¼˜åŒ–
1. é˜…è¯» `train/deepspeed_trainer.py` å’Œ DeepSpeed é…ç½®æ–‡ä»¶
2. ç†è§£ ZeRO Stage 1/2/3 çš„åˆ†ç‰‡ç­–ç•¥
3. å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹å¹¶åˆ†æå†…å­˜èŠ‚çœ
4. ä½¿ç”¨ Profiler è¯Šæ–­è®­ç»ƒç“¶é¢ˆ

---

## ğŸ“– å‚è€ƒèµ„æ–™

æœ¬é¡¹ç›®åŸºäºä»¥ä¸‹æ ¸å¿ƒæŠ€æœ¯å’Œè®ºæ–‡ï¼š

- **Transformer åŸç†**ï¼š*Attention Is All You Need* (Vaswani et al., 2017)
- **DDP æœºåˆ¶**ï¼š[PyTorch Distributed Overview](https://docs.pytorch.org/tutorials/beginner/dist_overview.html)
- **torchrun å¯åŠ¨å™¨**ï¼š[Fault-tolerant Distributed Training](https://docs.pytorch.org/tutorials/beginner/ddp_series_fault_tolerance.html)
- **DeepSpeed ZeRO**ï¼š[ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)
- **æ¢¯åº¦æ£€æŸ¥ç‚¹**ï¼š[Training Deep Nets with Sublinear Memory Cost](https://arxiv.org/abs/1604.06174)

---

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

åœ¨æäº¤ä»£ç å‰ï¼Œè¯·ç¡®ä¿ï¼š
1. ä»£ç éµå¾ª PEP 8 è§„èŒƒ
2. æ·»åŠ å¿…è¦çš„æ³¨é‡Šå’Œæ–‡æ¡£å­—ç¬¦ä¸²
3. é€šè¿‡åŸºç¡€åŠŸèƒ½æµ‹è¯•

---

## ğŸ“œ License

MIT License

---

## ğŸ™ è‡´è°¢

æ„Ÿè°¢ PyTorchã€DeepSpeed å’Œ Hugging Face ç¤¾åŒºçš„å¼€æºè´¡çŒ®ã€‚

æœ¬é¡¹ç›®çš„è®¾è®¡çµæ„Ÿæ¥æºäºã€ŠåŸºäº PyTorch 1.10+ã€torchrun ä¸ DeepSpeed çš„å¤§è§„æ¨¡ Transformer è®­ç»ƒå·¥ç¨‹æŒ‡å—ã€‹ã€‚

